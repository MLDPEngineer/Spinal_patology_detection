{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import os\n",
    "import random\n",
    "from lxml import etree\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_DIR = './DataSets/images/'\n",
    "METADATA_DIR = './DataSets/descr/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4670297cd7e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGES_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmarcup_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETADATA_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./descr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './images'"
     ]
    }
   ],
   "source": [
    "image_files = [os.path.abspath(IMAGES_DIR+file_name) for file_name in os.listdir(IMAGES_DIR)]\n",
    "marcup_files = [os.path.abspath(METADATA_DIR+file_name) for file_name in os.listdir(METADATA_DIR)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Parsing marcup files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consolodate all csv markup files\n",
    "markup = df()\n",
    "for file in marcup_files:\n",
    "    markup = pd.concat([markup, pd.read_csv(file)], axis=0)\n",
    "\n",
    "#FILTER APPROVED MARKUPS\n",
    "approved_markup = markup.loc[markup['На срезе визуализируются межпозвоночные диски'] == \\\n",
    "           'Визуализируются (можно размечать)', ['Файлы', 'XML', 'Исследователь', 'Кейс']]\n",
    "\n",
    "approved_markup['Файлы'] = approved_markup['Файлы'].str.replace('/n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parse_XML(object):\n",
    "    \"\"\"XML parser file\n",
    "        can call: parse_XML(XML_string).get_child('tag').get_child('another_tag')\n",
    "    \"\"\"\n",
    "    def __init__(self, XML_string):\n",
    "        self.root = etree.fromstring(XML_string)\n",
    "        self.tree = etree.ElementTree(self.root)\n",
    "        self.parsed_list = self.root\n",
    "        \n",
    "    def get_child(self, tag):\n",
    "        if not isinstance(self.parsed_list, list):\n",
    "            self.parsed_list = self.root.findall(tag)\n",
    "        else:\n",
    "            self.parsed_list = list(map(lambda lst: lst.findall(tag), self.parsed_list))\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        return list(map(lambda x: x.text(), self.parsed_list))\n",
    "    \n",
    "    def show_structure(self):\n",
    "        trees = []\n",
    "        for tag in self.parsed_list.iter():\n",
    "            path = self.tree.getpath(tag)\n",
    "            w_space = re.findall(r'[/]', path)\n",
    "            line = '    '*len(w_space)+tag.tag\n",
    "            if line not in trees:\n",
    "                trees.append(line)\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xml = parse_XML(approved_markup.iloc[4,1])\n",
    "test_xml.show_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_childs = lambda elements: list(map(lambda lst: lst.text, elements))\n",
    "\n",
    "def get_element_val(root, XPath):\n",
    "    \"\"\"Finding all element values by Xpath\"\"\"\n",
    "    elements = root.findall(XPath)\n",
    "    return get_childs(elements)\n",
    "\n",
    "parsed_xml = df(columns =['Files','Target names', 'B boxes', 'Img size rows', 'Img size cols', \"Researcher\"])\n",
    "#PARSE ALL XML MARKUP FILE STRING\n",
    "for i, row in approved_markup.iterrows():\n",
    "    try:\n",
    "        root = ET.fromstring(row['XML'])\n",
    "        #TARGET NAMES\n",
    "        target_names = list(get_element_val(root, \".//*/{http://www.w3.org/1999/xhtml}name\"))\n",
    "        #BOUNDING BOXES\n",
    "        X_coordinate = list(map(int, get_element_val(root, \".//*/{http://www.w3.org/1999/xhtml}x\")))\n",
    "        Y_coordinate = list(map(int, get_element_val(root, \".//*/{http://www.w3.org/1999/xhtml}y\")))\n",
    "        bb_points = list(zip(X_coordinate, Y_coordinate)) #bounding box points\n",
    "        b_boxes = [bb_points[i:i+4] for i in range(0,len(bb_points),4)] #bounding box 4 points coordinated (upper_left, upper_right, down_left, down_right)\n",
    "        #IMAGE SIZE\n",
    "        im_size_rows = get_element_val(root, \".//*/nrows\")*len(b_boxes)\n",
    "        im_size_cols = get_element_val(root, \".//*/ncols\")*len(b_boxes)\n",
    "        \n",
    "        parsed_xml = pd.concat([parsed_xml, df({'Files':row['Файлы'], \n",
    "                                                'Target names':target_names, \n",
    "                                                'B boxes': b_boxes, \n",
    "                                                'Img size rows':im_size_rows, \n",
    "                                                'Img size cols':im_size_cols,\n",
    "                                                'Researcher':row['Исследователь']\n",
    "                                               })])\n",
    "    except:\n",
    "        print(i, ' not XML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_xml.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>ANALYZE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(set(parsed_xml['Files']))\n",
    "markup_persons_num = len(set(approved_markup['Исследователь']))\n",
    "\n",
    "print(\"Размер датасета: {0}\\nРазмер картинок {1}\\nКол-во файлов размеченных людьми {2}\".\n",
    "      format(dataset_size, \n",
    "             Counter(parsed_xml['Img size cols']), \n",
    "             Counter(approved_markup['Исследователь'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILE DUPLICATES\n",
    "image_dublicates = Counter(approved_markup['Файлы']).most_common(3)\n",
    "image_dublicates = list(zip(*image_dublicates))\n",
    "image_dublicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique target names with counts\n",
    "Counter(parsed_xml['Target names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поскольку датасет небольшой и задача классификации стоит бинарная то:\n",
    "- обьединим все патологические классы в один класс, а здоровые в другой класс \n",
    "- бинаризируем: 1-больной класс, 0 здоровый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ill_cond = (parsed_xml['Target names']=='hernia') \\\n",
    "    | (parsed_xml['Target names']=='protrusion') \\\n",
    "    | (parsed_xml['Target names']=='shejnyj-mezhpozvonochnyj-disk-patologicheskij') \\\n",
    "    | (parsed_xml['Target names']=='grudnoj-mezhpozvonochnyj-disk-s-podozreniem-na-patalogiyu') \\\n",
    "    | (parsed_xml['Target names']=='grudnoj-mezhpozvonochnyj-disk-patologicheskij') \\\n",
    "    | (parsed_xml['Target names']=='shejnyj-mezhpozvonochnyj-disk-s-podozreniem-na-patologiyu')\n",
    "\n",
    "health_cond = (parsed_xml['Target names']=='shejnyj-mezhpozvonochnyj-disk-zdorovyj') \\\n",
    "    | (parsed_xml['Target names']=='grudnoj-mezhpozvonochnyj-disk-zdorovyj')\n",
    "\n",
    "parsed_xml.loc[ill_cond, 'Target names'] = 1\n",
    "parsed_xml.loc[health_cond, 'Target names'] = 0\n",
    "Counter(parsed_xml['Target names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_meta(file_name):\n",
    "    \"\"\"Load images from disk and get bounding boxes from XML description\"\"\"\n",
    "    img_obj = mpimg.imread(os.path.abspath(IMAGES_DIR+file_name))\n",
    "    b_box_target = parsed_xml.loc[parsed_xml['Files']==file_name, ['B boxes', 'Target names']]\n",
    "    return img_obj, b_box_target['B boxes'], b_box_target['Target names'].tolist()\n",
    "\n",
    "get_random_image_name = lambda : random.choice(list(parsed_xml['Files']))\n",
    "\n",
    "def plot_img_classes(row=1, col=2, show_taret=False, image=None):\n",
    "    \"\"\"Plot random images with bounding boxes (BB) and show target by BB color\"\"\"\n",
    "    fig, ax = plt.subplots(row, col, figsize=(25,15))\n",
    "    axs = ax.flatten()\n",
    "    for a_x in axs:\n",
    "        if image==None:\n",
    "            img, b_box, target = get_image_meta(get_random_image_name())\n",
    "        else:\n",
    "            img, b_box, target = get_image_meta(image)\n",
    "        if show_taret:\n",
    "            for i, coordinates in enumerate(b_box):\n",
    "                bb_height = coordinates[2][1] - coordinates[1][1]\n",
    "                bb_width = coordinates[2][0] - coordinates[0][0]\n",
    "                color='r'; color='r' if target[i] == 1 else 'g'\n",
    "                a_x.add_patch(Rectangle(coordinates[0], bb_width, bb_height, fill=False, linewidth=2, edgecolor=color, facecolor=color))\n",
    "        a_x.imshow(img, origin='upper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img_classes(row=3, col=5, show_taret=False)\n",
    "# plt.savefig('spinal_shots_map.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img_classes(row=1, col=2, show_taret=True, image='img_01280.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img_classes(row=1, col=2, show_taret=True)\n",
    "# plt.savefig('spinal_shots_markup.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- все снимки сделаны с одного ракурса (оператор устанавливает пациента в аппарат строго в определенном ракурсе) и вариативность признаков очень малеьнкая, что хорошо\n",
    "- по этой же причине большого смысла в агументации в виде поворотов снимков нет\n",
    "- размер картинок разный\n",
    "- колво людей делающих разметку 3 человека, это возможно мало для точного результата (проверку на пересечения для упрощения не делаю)  \n",
    "- данных очень мало (356 размеченных картинок) и на серьезный результат расчитывать не приходиться\n",
    "- присутствуют дубликаты картинок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>CREATE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TARGET CLASS DISTRIBUTION\n",
    "parsed_xml[['Target names', 'Img size rows', 'Img size cols']] = parsed_xml[['Target names', 'Img size rows', 'Img size cols']].astype(int)\n",
    "parsed_xml['Target names'].hist(bins=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Меняем последовательность координат на правильную"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за особенностей разметки - выделения полигонов людьми размечающими данные (например: квадрат может быть выделен снизу вверх или сверху вниз или сбоку),коррдинаты точек могут идти в разной последовательности.\n",
    "Необходимо их привести к следующей нотации: \n",
    "- 1-я координата - верхняя левая\n",
    "- 2-я верхняя правая\n",
    "- 3-я нижняя правая\n",
    "- 4-я нижняя левая\n",
    "<br>при условии origin='upper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK CONSISTENCE OF COORDINATES\n",
    "# P0 *-----* P1\n",
    "#    |     |\n",
    "#    |     |\n",
    "# P3 *-----* P2\n",
    "from operator import itemgetter\n",
    "\n",
    "max_LOT = lambda lst: max(lst,key=itemgetter(1)) # List Of Tuple\n",
    "min_LOT = lambda lst: min(lst,key=itemgetter(1))\n",
    "\n",
    "not_consistence = lambda list_of_P:  list_of_P[0]!=min(list_of_P) and list_of_P[2]!=max(list_of_P)\n",
    "not_consistence_count = lambda x: sum(list(map(not_consistence, x)))\n",
    "print('Кол-во неконсистентных координат: ', not_consistence_count(parsed_xml['B boxes']))\n",
    "\n",
    "\n",
    "parsed_xml['B boxes'] = \\\n",
    "                      [[(min(list(zip(*P))[0]), min(list(zip(*P))[1])), \n",
    "                      (max(list(zip(*P))[0]), min(list(zip(*P))[1])), \n",
    "                       (max(list(zip(*P))[0]), max(list(zip(*P))[1])), \n",
    "                      (min(list(zip(*P))[0]), max(list(zip(*P))[1])) ] for i,P in parsed_xml['B boxes'].items()]\n",
    "\n",
    "assert not_consistence_count(parsed_xml['B boxes']) == 0, 'Порядок точек не консистентен'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Балансировка выборки:\n",
    "выборку необходимо сбалансировать по следующим признакам:\n",
    "- таргет классы\n",
    "- люди размечающие данные\n",
    "- дублированным файлам\n",
    "- глубине снимка (для упрощения делать не буду)\n",
    "- машине делающей снимок (для упрощения делать не буду)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIND IMAGES WITH THE SAME COUNTS OF TARGET CLASSES \n",
    "image_clases = parsed_xml.loc[:,['Files', 'Target names']].groupby(by='Files').sum()\n",
    "image_clases.rename(columns={'Target names': 'Target sum'}, inplace=True)\n",
    "#ADD IMAGE CLASES TO parsed_xml\n",
    "parsed_xml = pd.merge(parsed_xml, image_clases, how='left', on='Files')\n",
    "print(\"Частота Таргет класса в разметке каждой картинки\")\n",
    "Counter(image_clases['Target sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOW MANY TIMES RESEARCHERS MARK EACH TARGET CLASS\n",
    "print(pd.crosstab(parsed_xml['Target names'], parsed_xml['Researcher']))\n",
    "parsed_xml['Researcher'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " На этом шаге создаем псевдо класс который включает всебя субклассы: \n",
    " - Таргет классы, \n",
    " - Исследователи, \n",
    " - Кол-во таргет классов на одной картинке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDING PSEUDO CLASS WITCH CONSOLIDATE 3 classes for dataset balancing\n",
    "parsed_xml['Pseudo_classes'] = parsed_xml[['Target names', 'Researcher', 'Target sum']].apply(lambda x: str(x[0])+' '+x[1]+' '+str(x[2]), axis=1)\n",
    "pseudo_cl_distr = parsed_xml['Pseudo_classes'].value_counts()\n",
    "print('Total classes: {0}\\nPseudoclass counts:\\n\\n{1}'.format(len(pseudo_cl_distr), pseudo_cl_distr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подгоняем размеры полигонов под размер сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_xml['B boxes'] = [[(int(v[0]*224/item['Img size rows']), \n",
    "#                            int(v[1]*224/item['Img size cols'])) for v in item['B boxes']] \n",
    "#                                                                 for i, item in parsed_xml.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделяем дубликаты картинок в трейн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIND DUBLICATED IMAGES\n",
    "dublicated_img = parsed_xml.loc[parsed_xml['Files'].isin(list(image_dublicates[0])),:]\n",
    "#drop dublicates\n",
    "parsed_xml = parsed_xml.loc[~parsed_xml['Files'].isin(list(image_dublicates[0])),:] \n",
    "#EXTRACT PSEUDOCLASS WITH 1 INSTANCE\n",
    "train_pseudo_class = parsed_xml.loc[parsed_xml['Pseudo_classes']=='0 Ilya Gusev 9']\n",
    "#drop\n",
    "parsed_xml = parsed_xml.loc[parsed_xml['Pseudo_classes']!='0 Ilya Gusev 9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(parsed_xml, parsed_xml['Target names'], stratify=parsed_xml['Pseudo_classes'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train, dublicated_img, train_pseudo_class, ], axis=0)\n",
    "y_train = X_train['Target names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset for Model data generator\n",
    "#### in CSV format: \"path/to/image.jpg,x1,y1,x2,y2,class_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset = lambda data_frame: \\\n",
    "    df({\n",
    "        'Image file': data_frame['Files'].map(lambda file_name: IMAGES_DIR+file_name),\n",
    "                'x1': data_frame['B boxes'].map(lambda x: x[0][0]),\n",
    "                'y1': data_frame['B boxes'].map(lambda x: x[0][1]),\n",
    "                'x2': data_frame['B boxes'].map(lambda x: x[2][0]),\n",
    "                'y2': data_frame['B boxes'].map(lambda x: x[2][1]),\n",
    "        'Class name': data_frame['Target names'].map(lambda x: 'Healthy' if x==0 else 'Ill')\n",
    "    })\n",
    "\n",
    "create_dataset(X_train).to_csv('./DataSets/train_annotations.csv', header=False, index=False)\n",
    "create_dataset(X_train).to_csv('./DataSets/valid_annotations.csv', header=False, index=False)\n",
    "data_frame = df({'Class': ['Healthy', 'Ill'], 'Id':[0,1]}).to_csv('./DataSets/classes.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
